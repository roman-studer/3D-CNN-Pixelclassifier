{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-02T08:31:16.360954800Z",
     "start_time": "2024-02-02T08:31:16.270471500Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS\n",
    "from torchsummary import summary\n",
    "import lightning as L\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import spectral\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class HyperspectralDataset(Dataset):\n",
    "    def __init__(self, cube_dir, mask_dir, window_size):\n",
    "        self.cube_dir = cube_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.window_size = window_size\n",
    "        self.p = window_size // 2\n",
    "        # list subfolders starting with E\n",
    "        self.cube_files = [i.split('\\\\')[-1] for i in glob(os.path.join(cube_dir, 'E*'))]\n",
    "        self.current_cube = None\n",
    "        self.current_mask = None\n",
    "        self.current_cube_index = -1\n",
    "        self.image_shape = None\n",
    "        self.n_pc = 15\n",
    "        self.window_indices = self.prepare_window_indices()\n",
    "        self.gradient_mask = self.get_gradient_mask()\n",
    "\n",
    "\n",
    "    def prepare_window_indices(self):\n",
    "        window_indices = []\n",
    "        for cube_index, cube_file in enumerate(self.cube_files):\n",
    "            cube_path = os.path.join(self.cube_dir, cube_file, 'hsi.npy')\n",
    "            cube = np.load(cube_path)\n",
    "            #cube = np.transpose(cube, (1, 0, 2))\n",
    "            self.image_shape = cube.shape[:2]\n",
    "\n",
    "            num_windows_x = cube.shape[0] // self.window_size\n",
    "            num_windows_y = cube.shape[1] // self.window_size\n",
    "\n",
    "            for i in range(num_windows_x):\n",
    "                for j in range(num_windows_y):\n",
    "                    window_indices.append((cube_index, i * self.window_size, j * self.window_size))\n",
    "        return window_indices\n",
    "\n",
    "    def load_cube(self, cube_index):\n",
    "        if cube_index != self.current_cube_index:\n",
    "            cube_path = os.path.join(self.cube_dir, self.cube_files[cube_index])\n",
    "\n",
    "            mask_all = np.zeros(self.image_shape)\n",
    "            hsi_masks = glob(os.path.join(self.mask_dir, self.cube_files[cube_index], 'hsi_masks/*bmp'))\n",
    "            # TODO: if more classes are introduced mask needs to be int and not bool\n",
    "            for mask_file in hsi_masks:\n",
    "                # load image with PIL\n",
    "                mask = Image.open(mask_file)\n",
    "                mask = np.array(mask)\n",
    "                mask = cv2.resize(mask, (self.image_shape[1], self.image_shape[0]))\n",
    "                mask_all = np.logical_or(mask_all, mask)\n",
    "\n",
    "            self.current_cube = np.load(os.path.join(cube_path, 'hsi.npy'))\n",
    "            self.pre_process_cube()\n",
    "            self.current_cube = np.pad(self.current_cube, ((self.p, self.p), (self.p, self.p), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "            self.current_mask = mask_all\n",
    "            self.current_mask = np.pad(self.current_mask, ((self.p, self.p), (self.p, self.p)), mode='constant', constant_values=0)\n",
    "            self.current_mask = self.current_mask.astype(int)\n",
    "            self.current_cube_index = cube_index\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    def pre_process_cube(self):\n",
    "        # TODO: implement pca, random occlusion, gradient masking (if necessary)\n",
    "        self.crop_bands()\n",
    "        self.remove_background()\n",
    "        self.apply_pca()\n",
    "        pass\n",
    "        \n",
    "    def apply_pca(self):\n",
    "        \"\"\"Applies PCA to cube and reduces number of bands to 15.\n",
    "        \n",
    "        Note: \n",
    "            - Function assumes that edge bands are removed, i.e. spectra are cropped. \n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.current_cube.reshape(-1, self.current_cube.shape[-1])\n",
    "        x = x.astype(float)\n",
    "        \n",
    "        # TODO: check for optimal number of components\n",
    "        pca = PCA(n_components=self.n_pc)\n",
    "        x = pca.fit_transform(x)\n",
    "        x = x.reshape(self.current_cube.shape[0], self.current_cube.shape[1], 15)\n",
    "        \n",
    "        self.current_cube = x\n",
    "        \n",
    "    def remove_background(self):\n",
    "        \"\"\"Sets spectra with mean intensity below 600 to zero on all bands. Treats overall low intensity spectra as background. \n",
    "        \n",
    "        Note: \n",
    "            - Changes in light intensity between cubes are not considered.\n",
    "            - Function assumes that edge bands are removed, i.e. spectra are cropped. \n",
    "        \"\"\"\n",
    "        mean_intensity = np.mean(self.current_cube, axis=2)\n",
    "        self.current_cube[mean_intensity < 600] = 0\n",
    "        \n",
    "    def crop_bands(self):\n",
    "        \"\"\"Removes bands 0-8 and 210-224. Assumes cube is of shape (w, h, 224).\n",
    "        \n",
    "        Note: \n",
    "            - Function assumes that edge bands are removed, i.e. spectra are cropped. \n",
    "        \"\"\"\n",
    "        self.current_cube = self.current_cube[:, :, 8:210]\n",
    "        \n",
    "    \n",
    "    def apply_gradient_mask(self, window):\n",
    "        \"\"\"Applies gradient mask to window as described in https://www.mdpi.com/2072-4292/15/12/3123\"\"\"\n",
    "        return window * self.gradient_mask\n",
    "\n",
    "    def get_gradient_mask(self):\n",
    "        s = self.window_size\n",
    "        p = self.n_pc\n",
    "        center = (s + 1) / 2\n",
    "        mask = np.zeros((s, s))\n",
    "        for i in range(s):\n",
    "            for j in range(s):\n",
    "                mask[i, j] = 1 - ((i - center + 1)**2 + (j - center + 1)**2) / (2 * center**2)\n",
    "\n",
    "        mask = np.expand_dims(mask, axis=2)\n",
    "        mask = np.repeat(mask, p, axis=2)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cube_index, i, j = self.window_indices[idx]\n",
    "        self.load_cube(cube_index)\n",
    "        window = self.current_cube[i:i+self.window_size, j:j+self.window_size, :].astype(np.float32)\n",
    "        window = np.transpose(window, (2, 0, 1))\n",
    "        window_mask = self.current_mask[i:i+self.window_size, j:j+self.window_size]\n",
    "\n",
    "        return window, window_mask\n",
    "\n",
    "\n",
    "class HyperspectralDataModule(LightningDataModule):\n",
    "    def __init__(self, cube_dir, mask_dir, window_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.cube_dir = cube_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = HyperspectralDataset(self.cube_dir, self.mask_dir, self.window_size)\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = HyperspectralDataset(self.cube_dir, self.mask_dir, self.window_size)\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:23:03.961901Z",
     "start_time": "2024-02-02T12:23:03.942803200Z"
    }
   },
   "id": "3a7e4f5a002a3c7e",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class HyperSN(L.LightningModule):\n",
    "    \"\"\"Implementation of HyperSN for Hyperspectral Cubes (3D Conv) from https://ieeexplore.ieee.org/document/8736016 based on https://github.com/Pancakerr/HybridSN/blob/master/HybridSN.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, patch_size, class_nums):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.class_nums = class_nums\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv3d(1, out_channels=8, kernel_size=(7,23,23), padding=(1,1,1)),\n",
    "                                   nn.BatchNorm3d(8),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv3d(8, out_channels=16, kernel_size=(5,21,21), padding=(1,1,1)),\n",
    "                                   nn.BatchNorm3d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv3d(16, out_channels=32, kernel_size=(3,19,19), padding=(1,1,1)),\n",
    "                                   nn.BatchNorm3d(32),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.x1_shape = self.get_shape_after_3dconv()\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(self.x1_shape[1]*self.x1_shape[2], out_channels=64, kernel_size=(3,3), padding=(1,1)),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.x2_shape = self.get_shape_after_2dconv()\n",
    "        \n",
    "        self.dense1 = nn.Sequential(nn.Linear(self.x2_shape, 1024),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Dropout(0.4))\n",
    "        self.dense2 = nn.Sequential(nn.Linear(1024, 128),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Dropout(0.4))\n",
    "        self.dense3 = nn.Linear(128, self.class_nums)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        y = mask[:, self.patch_size//2, self.patch_size//2].long()  # Convert to Long\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=self.class_nums).float()\n",
    "\n",
    "\n",
    "        x.unsqueeze_(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.shape[0],x.shape[1]*x.shape[2],x.shape[3],x.shape[4])\n",
    "        x = self.conv4(x)\n",
    "        x = x.contiguous().view(x.shape[0], -1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        out = self.dense3(x)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(out, y)\n",
    "        return loss\n",
    "                                    \n",
    "\n",
    "    def get_shape_after_2dconv(self):\n",
    "        x = torch.zeros((1, self.x1_shape[1]*self.x1_shape[2], self.x1_shape[3], self.x1_shape[4]))\n",
    "        with torch.no_grad():\n",
    "            x = self.conv4(x)\n",
    "        return x.shape[1]*x.shape[2]*x.shape[3]\n",
    "    \n",
    "    def get_shape_after_3dconv(self):\n",
    "        x = torch.zeros((1, 1, self.in_channels, self.patch_size, self.patch_size))\n",
    "        with torch.no_grad():\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "        return x.shape\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "    \n",
    "    \n",
    "model = HyperSN(in_channels=15, patch_size=128, class_nums=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:23:06.144896900Z",
     "start_time": "2024-02-02T12:23:04.598977700Z"
    }
   },
   "id": "c1a3e155b5502b4f",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cube_dir = '../../biocycle/data/processed/bcd_val/data/'\n",
    "mask_dir = '../../biocycle/data/processed/bcd_val/data/'\n",
    "window_size = 128\n",
    "batch_size = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:23:06.152089500Z",
     "start_time": "2024-02-02T12:23:06.146659700Z"
    }
   },
   "id": "9034cd7ae15c13e9",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_module = HyperspectralDataModule(cube_dir, mask_dir, window_size, batch_size)\n",
    "train_loader = data_module.train_dataloader()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:23:08.394813700Z",
     "start_time": "2024-02-02T12:23:06.150076100Z"
    }
   },
   "id": "8264ddfca509e285",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# wandb_logger = WandbLogger(project='pixelclassifier_hyperSN', entity='biocycle', log_model=\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:23:08.401500100Z",
     "start_time": "2024-02-02T12:23:08.395813700Z"
    }
   },
   "id": "ec73df6fa62df55e",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | conv1  | Sequential | 29.6 K\n",
      "1 | conv2  | Sequential | 282 K \n",
      "2 | conv3  | Sequential | 554 K \n",
      "3 | conv4  | Sequential | 166 K \n",
      "4 | dense1 | Sequential | 358 M \n",
      "5 | dense2 | Sequential | 131 K \n",
      "6 | dense3 | Linear     | 258   \n",
      "--------------------------------------\n",
      "360 M     Trainable params\n",
      "0         Non-trainable params\n",
      "360 M     Total params\n",
      "1,440.161 Total estimated model params size (MB)\n",
      "C:\\Users\\stude\\anaconda3\\envs\\biocycling_px\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e99837380ef24d7d9d6457af07cd2b80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stude\\anaconda3\\envs\\biocycling_px\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# trainer = L.Trainer(max_epochs=1, logger=wandb_logger)\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "trainer.fit(model, train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:23:33.817343100Z",
     "start_time": "2024-02-02T12:23:08.401500100Z"
    }
   },
   "id": "962e24e98bdf546",
   "execution_count": 88
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
